{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Happiness Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "### Context\n",
    "\n",
    ">The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.\n",
    "\n",
    "### Content\n",
    "\n",
    ">The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors – \n",
    "\n",
    "                                              1. Economic production, \n",
    "                                              2. Social support, \n",
    "                                              3. Life expectancy, \n",
    "                                              4. Freedom, \n",
    "                                              5. Absence of corruption, \n",
    "                                              6. Generosity\n",
    "                                              \n",
    ">Contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.\n",
    "\n",
    "### Inspiration\n",
    "\n",
    ">What countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? Did any country experience a significant increase or decrease in happiness?\n",
    "\n",
    "### What is Dystopia?\n",
    "\n",
    ">Dystopia is an imaginary country that has the world’s least-happy people. The purpose in establishing Dystopia is to have a benchmark against which all countries can be favorably compared (no country performs more poorly than Dystopia) in terms of each of the six key variables, thus allowing each sub-bar to be of positive width. The lowest scores observed for the six key variables, therefore, characterize Dystopia. Since life would be very unpleasant in a country with the world’s lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom and least social support, it is referred to as “Dystopia,” in contrast to Utopia.\n",
    "\n",
    "### What are the residuals?\n",
    "\n",
    ">The residuals, or unexplained components, differ for each country, reflecting the extent to which the six variables either over- or under-explain average 2014-2016 life evaluations. These residuals have an average value of approximately zero over the whole set of countries. Figure 2.2 shows the average residual for each country when the equation in Table 2.1 is applied to average 2014- 2016 data for the six variables in that country. We combine these residuals with the estimate for life evaluations in Dystopia so that the combined bar will always have positive values. As can be seen in Figure 2.2, although some life evaluation residuals are quite large, occasionally exceeding one point on the scale from 0 to 10, they are always much smaller than the calculated value in Dystopia, where the average life is rated at 1.85 on the 0 to 10 scale.\n",
    "\n",
    "### What do the columns succeeding the Happiness Score(like Family, Generosity, etc.) describe?\n",
    "\n",
    ">The following columns: GDP per Capita, Family, Life Expectancy, Freedom, Generosity, Trust Government Corruption describe the extent to which these factors contribute in evaluating the happiness in each country.\n",
    "The Dystopia Residual metric actually is the Dystopia Happiness Score(1.85) + the Residual value or the unexplained value for each country as stated in the previous answer.\n",
    "\n",
    ">If you add all these factors up, you get the happiness score so it might be un-reliable to model them to predict Happiness Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing reqiredd libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 109, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8318d97bbf4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://github.com/dsrscientist/DSData/blob/master/happiness_score_dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 109, saw 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://github.com/dsrscientist/DSData/blob/master/happiness_score_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are no null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "num_cols = df._get_numeric_data().columns\n",
    "\n",
    "catagorical_data =list(set(cols) - set(num_cols))\n",
    "catagorical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Data with Label Encoder converting all the catagorical data into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE= LabelEncoder()\n",
    "\n",
    "\n",
    "for i in catagorical_data:\n",
    "    df[i]=LE.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see the mathematical Summary of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key observation:\n",
    "\n",
    "1. Happiness Rank.\n",
    "2. Country. \n",
    ">have only unique value in all the columns so it will not help us in prediction\n",
    "\n",
    "1. Happiness Score.\n",
    "2. Standard Error.\n",
    "3. Family \n",
    "4. Health (life Expectancy)\n",
    "5. Freedom\n",
    "6. Trust\n",
    "7. Generosity\n",
    "8. Dystopia Residual\n",
    ">Continous data have the mean and 50th percentile nearly equal and difference between 75th percentile and max is little.\n",
    "The above observations define Skewness is lesser and less outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Happiness Rank\",\"Country\"], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeness Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "collist = df.columns.values\n",
    "for i in range (0, len(collist)):\n",
    "    plt.subplot(2,5,i+1)         \n",
    "    sns.kdeplot(df[collist[i]], color = \"purple\")\n",
    "    plt.title(f\"Skewness = {round(df[collist[i]].skew(),5)}\",fontsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(edgecolor=\"red\",linewidth= 1.5, figsize= (20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness=[]\n",
    "for i in df.skew().values:\n",
    "    skewness.append(i)\n",
    "    \n",
    "df_skewness= pd.DataFrame({\"Feature_names\": collist,\"Skew\": skewness})\n",
    "df_skewness= df_skewness.sort_values(by=\"Skew\", ascending=False, ignore_index= True)\n",
    "\n",
    "\n",
    "skew_postive_row= []\n",
    "skew_negative_row=[]\n",
    "for index, row in df_skewness.iterrows():\n",
    "    if row['Skew']>0.49:\n",
    "        skew_postive_row.append(row['Feature_names'])\n",
    "    elif row['Skew']< -0.49:\n",
    "        skew_negative_row.append(row['Feature_names'])\n",
    "        \n",
    "df_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFeature names with Skewness is present more than +/-0.5 as follows:\\n\",\"\\n\\nPostive Skewed data:\\n\", skew_postive_row,\"\\n\\nnegative Skewed data:\\n\", skew_negative_row)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=df\n",
    "\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "for i in skew_postive_row:\n",
    "    DF[i]= yeojohnson(DF[i])[0]\n",
    "for i in skew_negative_row:\n",
    "    DF[i]= yeojohnson(DF[i])[0]   \n",
    "    \n",
    "print(\"BELOW GRAPH WILL SHOW THE SKEWNESS OF THE DATA\")\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range (0, len(collist)):\n",
    "    plt.subplot(8,4,i+1)\n",
    "    plt.title(f\"Skewness = {round(DF[collist[i]].skew(),5)}\",fontsize=20)         \n",
    "    sns.distplot(DF[collist[i]], color = \"#f80424\")\n",
    "    plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have removed the skewness of the data by \"yeojohnson\" method. We will remove the outliers and lets see the relationship of the data with the target variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DF\n",
    "plt.figure(figsize=(20,50))\n",
    "collist = df.columns.values\n",
    "for i in range (0, len(collist)):\n",
    "    plt.subplot(15,4,i+1)\n",
    "    ax=sns.boxplot(df[collist[i]], color = \"#fb0a29\" , orient = \"h\")\n",
    "    ax.set_facecolor(\"#fec1c9\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is certain amount of outliers exist in the data lets see them mathematically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "z= np.abs(zscore(df1))\n",
    "threshold= 3\n",
    "df_new = df[(z < 3).all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Orginal Data {df1.shape}\\nAfter Removing outliers {df_new.shape}\\nThe percentage of data loss {((158-155)/158)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key observation:\n",
    "    The outliers exist in the data post the removal of outliers the loss of data is very less so we can remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see the correlation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(),annot = True, cmap = \"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key observation:\n",
    "    Above we can see the happiness score has postive correlation with most of the variable and only region has negativa correlation.\n",
    "    this means increase in factors like GDP per capita, Family, freedom, life expentancy also increases hapiness score\n",
    "    Lets picturise the correlation with the targeet vaariable alone.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "df.corr()[\"Happiness Score\"].sort_values(ascending=False).drop([\"Happiness Score\"]).plot.bar()\n",
    "plt.xlabel(\"Feature\", fontsize= 14)\n",
    "plt.ylabel(\"correlation with Target column\", fontsize = 18)\n",
    "plt.title(\"Correlation of Fetures with the target column\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the ranking of the correlation of the feature variable with target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange =\"#60010d\"\n",
    "green =\"#980216\"\n",
    "grey =\"#fc6a7d\"\n",
    "yellow =\"#fecad1\"\n",
    "blue =\"#fc8292\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable_1 = \"Happiness Score\"\n",
    "feature_variable = ['Region',\"Standard Error\",\"Economy (GDP per Capita)\",\"Family\",\"Health (Life Expectancy)\",\"Freedom\",\n",
    "                    \"Trust (Government Corruption)\",\"Generosity\",\"Dystopia Residual\",\"Family\",\"Health (Life Expectancy)\",\n",
    "                    \"Freedom\",\"Trust (Government Corruption)\",\"Generosity\",\"Dystopia Residual\"]\n",
    "\n",
    "\n",
    "def num_plots(feature_name):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 2))\n",
    "    a1=sns.boxplot(x=df[feature_name], ax=axs[0], color=blue)\n",
    "    a1.set_facecolor(yellow)\n",
    "    a2=sns.distplot(df[feature_name], bins=20, kde=True, ax=axs[1],color=orange)\n",
    "    a2.set_facecolor(grey)\n",
    "    a3=sns.scatterplot(data=df, x=feature_name, y=target_variable_1, ax=axs[2], color=\"k\")\n",
    "    a3.set_facecolor(blue)\n",
    "    plt.show()\n",
    "    \n",
    "for i in feature_variable:\n",
    "    num_plots(i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key observation:\n",
    "         The distribution of the scattered plot is as per the correlation of the variable when the correlation is higher the distribution is lesser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key observation : \n",
    "    1. We have only 1 categorical variable \"Region\"we totally have 10 regions.\n",
    "    2. Most of the data have high postive correlation. The spread of the data is mostly diagonal down left to upper right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of the data wont be required since the vaules of the data is lesser lets split the data and stain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1=df.drop([\"Happiness Score\"], axis = 1)\n",
    "y_1=df[\"Happiness Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\n",
    "\n",
    "accu = 0\n",
    "for i in range(0,1000):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_1,y_1,test_size = .25, random_state = i)\n",
    "    mod = LinearRegression()\n",
    "    mod.fit(x_train,y_train)\n",
    "    y_pred = mod.predict(x_test)\n",
    "    tempacc = r2_score(y_test,y_pred)\n",
    "    if tempacc> accu:\n",
    "        accu= tempacc\n",
    "        best_rstate=i\n",
    "\n",
    "print(f\"Best Accuracy {accu*100} found on randomstate {best_rstate}\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_1,y_1,test_size = .25, random_state = best_rstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets shortlist promising Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), Lasso(), Ridge(alpha=1, random_state=42), ElasticNet(), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(), AdaBoostRegressor(random_state=42), RandomForestRegressor(random_state=42)]\n",
    "\n",
    "model_names = [\"LinearRegression\", \"Lasso\", \"Ridge\", \"ElasticNet\", \"SVR\", \"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"AdaBoostRegressor\", \"RandomForestRegressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score= []\n",
    "mean_abs_e=[]\n",
    "mean_sqr_e=[]\n",
    "root_mean_e=[]\n",
    "r2=[]\n",
    "\n",
    "for m in models:\n",
    "    m.fit(x_train_1,y_train_1)\n",
    "    print(\"Score of\", m, \"is:\", m.score(x_train_1,y_train_1))\n",
    "    score.append(m.score(x_train_1,y_train_1))\n",
    "    predm=m.predict(x_test_1)\n",
    "    print(\"\\nERROR:\")\n",
    "    print(\"MEAN ABSOLUTE ERROR: \",mean_absolute_error(y_test_1,predm))\n",
    "    mean_abs_e.append(mean_absolute_error(y_test_1,predm))\n",
    "    print(\"MEAN SQUARED ERROR: \", mean_squared_error(y_test_1,predm))\n",
    "    mean_sqr_e.append(mean_squared_error(y_test_1,predm))\n",
    "    print(\"ROOT MEAN SQUARED ERROR :\",np.sqrt(mean_squared_error(y_test_1,predm)))\n",
    "    root_mean_e.append(np.sqrt(mean_squared_error(y_test_1,predm)))\n",
    "    print(\"R2 SCORE: \", r2_score(y_test_1,predm))\n",
    "    r2.append(r2_score(y_test_1,predm))\n",
    "    print(\"**********************************************************************************************************\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score= []\n",
    "STD=[]\n",
    "for m in models:\n",
    "    CV=cross_val_score(m,x_1,y_1,cv=5,scoring=\"r2\")\n",
    "    print(\"SCORE OF\",m,\"IS:\")\n",
    "    print(\"SCORE IS:\", CV)\n",
    "    print(\"MEAN OF SCORE is :\", CV.mean())\n",
    "    mean_score.append(CV.mean())\n",
    "    print(\"Standard Deviation :\", CV.std())\n",
    "    STD.append(CV.std())\n",
    "    print(\"**************************************************************************************************\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression_result = pd.DataFrame({\"MODEL\": model_names,\n",
    "                                  \"SCORE\": score,\n",
    "                                  \"CV_mean_score\": mean_score,\n",
    "                                  \"CV_STD\": STD,\n",
    "                                  \"MBE\": mean_abs_e,\n",
    "                                  \"MSE\": mean_sqr_e,\n",
    "                                  \"RMSE\": root_mean_e,\n",
    "                                  \"R2\":r2 \n",
    "                                 })\n",
    "Regression_result.sort_values(by=\"CV_mean_score\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [\"SCORE\", \"CV_mean_score\", \"CV_STD\", \"MBE\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "\n",
    "for metric in metrics_list:\n",
    "    Regression_result.sort_values(by=metric).plot.bar(\"MODEL\", metric, color = orange)\n",
    "    plt.title(f\"MODEL by {metric}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above observation Linear Regression is undoubtedly the best model with the model score of 99% and also good CV score of 84% we are training our data with linear regression and saving the model without hypertuning since model score is 99%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(x_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(x_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(lr,\"happiness_score.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Happiness_Score = pd.DataFrame({\"pred_Happiness_Score\":pred})\n",
    "pred_Happiness_Score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Happiness_Score.to_csv(\"pred_Happiness_Score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    ">The above analysis is the observation of 157 countries hapiness scores and rank. Study which will help to understand the what are all the factors that increases the happiness score.\n",
    "    what is the contribution of GDP per capita and trust on the goverment that helps to improve the Quality of living.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
